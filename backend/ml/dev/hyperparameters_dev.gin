# Hyperparameters configuration file 


# ml_run parameters
ml_run.epochs_loc = 60  # (max) number of local training epochs of the algorithm
ml_run.epochs_glob = 60  # (max) number of global training epochs of the algorithm
ml_run.resume = False  # wether to resume training or not
ml_run.compute_uncertainty = False  # wether to compute uncertainty or not
                                        # (takes time)
ml_run.device = 'cpu'  # device used for computations ("cpu" or "cuda")


# Loss hyperparameters
# local
Licchavi.gamma = 0.1  # local regularisation term ponderation
# global
Licchavi.nu_par = 5  # scaling parameter term ponderation
Licchavi.w_loc = 1  # generalisation term ponderation
Licchavi.w0_par = 1  # regularisation term ponderation


# Training hyperparameters
# local
Licchavi.lr_loc = 0.1  # learning rate of local models
# global
Licchavi.lr_t = 0.005  # learning rate of t individual parameters
Licchavi.lr_s = 0.005  # learning rate of s individual parameters
Licchavi.lr_glob = 0.01  # learning rate of general model


# # learning rate scheduler  # FIXME adapt to new algorithm
# _lr_schedule.lr_rush_duration = 50  # duration of "rush phase" (nb of epochs)
# _lr_schedule.decay_rush = 0.97  # decay during "rush phase"
# _lr_schedule.decay_fine = 1  # decay during fine tuning phase
# _lr_schedule.min_lr_fine = 0.001  # minimum (local) learning rate

# _lr_schedule.precision = 0.97 #proportion of parameters at eq for early stopping
# _lr_schedule.epsilon = 0.1  # strength of equilibrium asked


# metrics to compute during training
Licchavi.metrics_loc = [
    'loss_fit', 'norm_loc', 'diff_loc',
] 
Licchavi.metrics_glob = [
    'loss_s', 'loss_gen', 'loss_reg', 'norm_glob', 'grad_sp', 
    'grad_norm', 'diff_glob', 'diff_s'
] 
