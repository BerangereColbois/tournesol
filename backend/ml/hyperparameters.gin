# Production hyperparameters configuration file 


# ml_run parameters
ml_run.resume = False  # wether to resume training or not
ml_run.compute_uncertainty = False  # wether to compute uncertainty or not
                                        # (takes time)
ml_run.device = 'cpu'  # device used for computations ("cpu" or "cuda")


# Loss hyperparameters
# local
Licchavi.gamma = 0.1  # local regularisation term ponderation
# global
Licchavi.nu_par = 0.0005  # scaling parameter term ponderation
Licchavi.w0_par = 0.001  # global regularisation term ponderation


# Training hyperparameters
# local
Licchavi.lr_loc = 0.1  # learning rate of local models
# global
Licchavi.lr_t = 0.9  # learning rate of t individual parameters
Licchavi.lr_s = 5  # learning rate of s individual parameters
Licchavi.lr_glob = 10  # learning rate of general model


# Training schedule
# if we resume training
ml_run.epochs_loc_res = 10  # (max) number of local training epochs of the algorithm
ml_run.epochs_glob_res = 10  # (max) number of global training epochs of the algorithm
# if we train from scratch
ml_run.epochs_loc_full = 60  # (max) number of local training epochs of the algorithm
ml_run.epochs_glob_full = 60  # (max) number of global training epochs of the algorithm


# metrics to compute during training
Licchavi.metrics_loc = [
    #  'loss_fit', 'norm_loc', 'diff_loc',
] 
Licchavi.metrics_glob = [
    # 'loss_s', 'loss_gen', 'loss_reg', 'norm_glob',
    # 'grad_glob', 'diff_glob', 'diff_s'
] 
