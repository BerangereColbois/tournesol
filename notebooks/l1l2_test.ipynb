{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import gin\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gin.enter_interactive_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression result with $l_2$ regularization and $l_1$ fit loss\n",
    "\n",
    "We solve for $L=\\|z-z_0\\|_1+\\mu\\|z-1\\|^2_2\\to\\min\\limits_{z}$. The first term mimicks the \"common to model\" loss term from the Tournesol loss, and the second term models the \"common to 1\" loss.\n",
    "\n",
    "The subgradient of the (1-dim) loss function is\n",
    "$$\n",
    "\\partial L=\\begin{cases}\n",
    "1+2\\mu(z-1),&z>z_0\\\\\n",
    "-1+2\\mu(z-1),&z<z_0\\\\\n",
    "[-1,1]+2\\mu(z-1),&z=z_0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The optimality condition $0\\in \\partial L$ gives us\n",
    "\n",
    "$z=\\begin{cases}\n",
    "z_0,&if z_0\\in 1+\\frac{1}{2\\mu}[-1,1]\\\\\n",
    "1-\\frac{1}{2\\mu},&if z_0<1-\\frac{1}{2\\mu}\\\\\n",
    "1+\\frac{1}{2\\mu},&if z_0>1+\\frac{1}{2\\mu}\\\\\n",
    "\\end{cases}$\n",
    "\n",
    "Which is equivalent to\n",
    "$$\n",
    "z=\\max\\left(1-\\frac{1}{2\\mu}, \\min\\left[1+\\frac{1}{2\\mu}, z_0\\right]\\right)= clamp(z_0, min=1-\\frac{1}{2\\mu}, max=1+\\frac{1}{2\\mu})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensionality\n",
    "n = 100\n",
    "z0 = np.linspace(-10, 10, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(z0)\n",
    "plt.xlabel('Dimension')\n",
    "plt.ylabel('Value z_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_z():\n",
    "    \"\"\"Get the trainable variable.\"\"\"\n",
    "    z = tf.Variable(tf.zeros(n))\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gin.configurable\n",
    "def loss(mu, z, z0):\n",
    "    \"\"\"Compute the loss function |z-z0|_1+mu|z-1|^2_2.\"\"\"\n",
    "    loss_fit = tf.reduce_sum(tf.abs(z - z0))\n",
    "    loss_reg = tf.reduce_sum(tf.square(z - 1))\n",
    "    loss_total = loss_fit + mu * loss_reg\n",
    "    return {'fit': loss_fit,\n",
    "            'reg': loss_reg,\n",
    "            'total': loss_total}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_step(z, opt):\n",
    "    \"\"\"One optimization step.\"\"\"\n",
    "    \n",
    "    z_vars = [z]\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        losses = loss(z=z)\n",
    "        loss_total = losses['total']\n",
    "\n",
    "    grads = tape.gradient(loss_total, z_vars)\n",
    "    opt.apply_gradients(zip(grads, z_vars))\n",
    "\n",
    "    losses['grad_norm'] = tf.linalg.norm(grads)\n",
    "\n",
    "    losses = {x: y.numpy() for x, y in losses.items()}\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_z_z0(z, z0, mu, show=True):\n",
    "    \"\"\"Plot z-vs-z0 scatter plot.\"\"\"\n",
    "    plt.plot(z0, z)\n",
    "    plt.xlabel('z0')\n",
    "    plt.ylabel('z')\n",
    "    plt.title(f'z vs z0, mu={round(mu, 2)}')\n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(z0, mu=0, epochs=25000):\n",
    "    \"\"\"One optimization experiment.\"\"\"\n",
    "    # learnable parameter\n",
    "    z = get_z()\n",
    "    \n",
    "    opt = tf.optimizers.Adam()\n",
    "    gin.bind_parameter('loss.z0', z0)\n",
    "    gin.bind_parameter('loss.mu', mu)\n",
    "    \n",
    "    losses = []\n",
    "\n",
    "    for _ in tqdm(range(epochs)):\n",
    "        losses.append(opt_step(z, opt))\n",
    "    \n",
    "    df = pd.DataFrame(losses)\n",
    "    \n",
    "    plt.figure(figsize=(13, 5))\n",
    "    n_plots = len(df.columns)\n",
    "    for i, col in enumerate(sorted(df.columns), 1):\n",
    "        plt.subplot(1, n_plots, i)\n",
    "        plt.plot(df[col])\n",
    "        plt.yscale('log')\n",
    "        plt.title(col)\n",
    "    plt.show()\n",
    "    \n",
    "    plot_z_z0(z.numpy(), z0, mu)\n",
    "    plt.show()\n",
    "        \n",
    "    return {'z': np.array(z.numpy()), 'losses': df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment(z0, mu=0.1, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying different mus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus = np.logspace(np.log10(0.02), np.log10(1), num=10)\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mu in tqdm(mus):\n",
    "    results.append(experiment(z0, mu=mu, epochs=25000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mu, z in zip(mus, [r['z'] for r in results]):\n",
    "    plt.plot(z0, z, label=mu)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(np.array([r['z'] for r in results]), yticklabels=mus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mu, z in zip(mus, [r['z'] for r in results]):\n",
    "    plt.plot(z0, z, label=mu)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?sns.heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
