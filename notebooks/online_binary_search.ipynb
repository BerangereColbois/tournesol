{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "# for adding the videos to DB\n",
    "# don't use at the same time with the server running\n",
    "# https://stackoverflow.com/questions/59119396/how-to-use-django-3-0-orm-in-a-jupyter-notebook-without-triggering-the-async-con\n",
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\"\n",
    "\n",
    "from backend.models import Video, UserPreferences, ExpertRating, DjangoUser, UserInformation, EmailDomain\n",
    "from backend.rating_fields import VIDEO_FIELDS\n",
    "import numpy as np\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import HTML, IFrame, display\n",
    "from IPython.display import YouTubeVideo\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import gin\n",
    "from backend.rating_fields import MAX_VALUE\n",
    "from matplotlib import pyplot as plt\n",
    "gin.enter_interactive_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_username = 'test_my_username'\n",
    "other_username = 'test_other_username'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating two certified users\n",
    "DjangoUser.objects.filter(username__startswith='test_').delete()\n",
    "dj_my = DjangoUser.objects.create(username=my_username)\n",
    "dj_other = DjangoUser.objects.create(username=other_username)\n",
    "up = UserPreferences.objects.create(user=dj_my)\n",
    "up_other = UserPreferences.objects.create(user=dj_other)\n",
    "ui = UserInformation.objects.create(user=dj_my)\n",
    "ui_other = UserInformation.objects.create(user=dj_other)\n",
    "domain, _ = EmailDomain.objects.get_or_create(domain='@online.com', status=EmailDomain.STATUS_ACCEPTED)\n",
    "VerifiableEmail.objects.create(user=ui, email='me@online.com', is_verified=True)\n",
    "VerifiableEmail.objects.create(user=ui_other, email='they@online.com', is_verified=True)\n",
    "user = up\n",
    "u = user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a 1000 videos\n",
    "Video.objects.filter(video_id__startswith='online').delete()\n",
    "n_videos = 10\n",
    "video_ids = [\"online-%05d\" % i for i in range(n_videos)]\n",
    "Video.objects.bulk_create([Video(video_id=vid) for vid in video_ids])\n",
    "video_qs = list(Video.objects.filter(video_id__in=video_ids))\n",
    "videos_by_id = {video.video_id: video for video in video_qs}\n",
    "videos = [videos_by_id[vid] for vid in video_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating some random ratings\n",
    "n_ratings = 10\n",
    "def get_n_random_pairs(n_videos, n_ratings):\n",
    "    pairs = set()\n",
    "    \n",
    "    def sample_pair():\n",
    "        v1 = np.random.choice(n_videos - 1) + 1\n",
    "        v2 = np.random.choice(v1)\n",
    "        pair = (v1, v2)\n",
    "        return pair\n",
    "    \n",
    "    for i in tqdm(range(n_ratings)):\n",
    "        pair = sample_pair()\n",
    "        while pair in pairs:\n",
    "            pair = sample_pair()\n",
    "        pairs.add(pair)\n",
    "    assert all([x > y for x, y in pairs])\n",
    "    assert len(pairs) == n_ratings\n",
    "    return list(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_ratings_from_pairs(videos, user, rating_pairs,\n",
    "                                     nonrandom_r=False):\n",
    "    to_create = []\n",
    "    \n",
    "    for v1, v2 in rating_pairs:\n",
    "        if np.random.rand() < 0.5:\n",
    "            (v1, v2) = (v2, v1)\n",
    "            \n",
    "        fields = {f: np.random.rand() * MAX_VALUE for f in VIDEO_FIELDS}\n",
    "        if nonrandom_r and np.random.rand() < 0.05:\n",
    "            delta = 1. * (v1 - v2) / len(videos) # value: -1, 1\n",
    "            delta += 1 # value: 0, 2\n",
    "            delta /= 2 # value: 0, 1\n",
    "            delta *= MAX_VALUE\n",
    "            if delta < 0:\n",
    "                delta = 0\n",
    "            if delta > MAX_VALUE:\n",
    "                delta = MAX_VALUE\n",
    "            fields[VIDEO_FIELDS[0]] = delta\n",
    "\n",
    "        to_create.append(ExpertRating(user=user, video_1=videos[v1],\n",
    "                                      video_2=videos[v2],\n",
    "                                      **fields))\n",
    "    \n",
    "    ExpertRating.objects.bulk_create(to_create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = get_n_random_pairs(n_videos, n_ratings)\n",
    "create_random_ratings_from_pairs(videos, user, pairs, nonrandom_r=True)\n",
    "\n",
    "pairs_other = get_n_random_pairs(n_videos, n_ratings)\n",
    "create_random_ratings_from_pairs(videos, up_other, pairs_other, nonrandom_r=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model...\n",
    "from backend.ml_model.client_server.django_ml_featureless import DatabasePreferenceLearnerFeatureless\n",
    "from django_react.settings import load_gin_config\n",
    "load_gin_config('../backend/backend/ml_model/config/featureless_config.gin')\n",
    "\n",
    "\n",
    "# running global fit...\n",
    "learner = DatabasePreferenceLearnerFeatureless(directory=None,\n",
    "                                     load=True, save=True)\n",
    "learner.fit(epochs=1000)\n",
    "learner.update_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = UserPreferences.objects.get(user__username=my_username)\n",
    "n_videos = Video.objects.filter(video_id__startswith='online').count()\n",
    "video_ids = [x[0] for x in Video.objects.filter(video_id__startswith='online').values_list('video_id')]\n",
    "video_ids = sorted(video_ids)\n",
    "videos_qs = Video.objects.filter(video_id__startswith='online')\n",
    "videos_dct = {video.video_id: video for video in videos_qs}\n",
    "videos = [videos_dct[video_id] for video_id in video_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature(f):\n",
    "    plt.xlabel('video id')\n",
    "    plt.ylabel(f'score [{f}]')\n",
    "    vrs = VideoRating.objects.filter(user__user__username=my_username)\n",
    "    feature_map = {int(vr.video.video_id.split('-')[1]): getattr(vr, f) for vr in vrs}\n",
    "    plt.plot([feature_map.get(i, -1) for i in range(n_videos)], label='my reliability')\n",
    "\n",
    "    vrs = VideoRating.objects.filter(user__user__username=other_username)\n",
    "    feature_map = {int(vr.video.video_id.split('-')[1]): getattr(vr, f) for vr in vrs}\n",
    "    plt.plot([feature_map.get(i, -1) for i in range(n_videos)], label='their reliability')\n",
    "\n",
    "    vrs = Video.objects.filter(video_id__startswith='online')\n",
    "    feature_map = {int(vr.video_id.split('-')[1]): getattr(vr, f) for vr in vrs}\n",
    "    plt.plot([feature_map[i] for i in range(n_videos)], label='total')\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_feature('reliability')\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_feature('backfire_risk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_by_me = Video.objects.annotate().filter(\n",
    "    Q(expertrating_video_1__user=u) | Q(expertrating_video_1__user=u)).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_by_me_and_others = [v for v in videos_by_me if v.rating_n_experts >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(videos_by_me_and_others), len(videos_by_me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_to_change = videos[np.random.choice(n_videos)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ExpertRating.objects.filter(video_1=video_to_change, user=u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_other_video(r, v):\n",
    "    if r.video_1 == v:\n",
    "        return r.video_2\n",
    "    elif r.video_2 == v:\n",
    "        return r.video_1\n",
    "    else:\n",
    "        raise Exception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_rating = ratings[np.random.choice(len(ratings))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_other = rating_other_video(random_rating, video_to_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating = ExpertRating.objects.get(video_1=video_to_change, video_2=v_other,\n",
    "                                  user=u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(VIDEO_FIELDS, rating.features_as_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating.video_1.name, rating.video_2.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'reliability'\n",
    "# OUR CHANGE\n",
    "\n",
    "OLD_VAL = getattr(rating, feature)\n",
    "NEW_VAL = 100\n",
    "\n",
    "print(\"CHANGING\", rating, feature, OLD_VAL, \"to\", NEW_VAL)\n",
    "\n",
    "setattr(rating, feature, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.ml_model.client_server.django_ml_featureless import DatabasePreferenceLearnerFeatureless\n",
    "from django_react.settings import load_gin_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_other = UserPreferences.objects.get(user__username=other_username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_to_ratings = {u.id: list(ExpertRating.objects.filter(\n",
    "    user__user__username=my_username).exclude(id=rating.id).order_by('?')[:10]) + [rating],\n",
    "                   u_other.id: list(ExpertRating.objects.filter(\n",
    "    user__user__username=other_username).order_by('?')[:10])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_adjacent_videos(video, filt, hops=3):\n",
    "    \"\"\"Get ratings that are related to the video in k hops.\"\"\"\n",
    "    queue = []  # format: (hops, video)\n",
    "    \n",
    "    visited = set()\n",
    "    \n",
    "    queue.append((0, video.id))\n",
    "    \n",
    "    while queue:\n",
    "        curr_hops, item = queue[0]\n",
    "        queue = queue[1:]\n",
    "        \n",
    "        if item in visited:\n",
    "            continue\n",
    "            \n",
    "        visited.add(item)\n",
    "        \n",
    "        curr_res = set()\n",
    "        related_1 = ExpertRating.objects.filter(filt & Q(video_1__id=item)).values('video_2__id')\n",
    "        for item_next in related_1:\n",
    "            curr_res.add(item_next['video_2__id'])\n",
    "        related_2 = ExpertRating.objects.filter(filt & Q(video_2=item)).values('video_1__id')\n",
    "        for item_next in related_2:\n",
    "            curr_res.add(item_next['video_1__id'])\n",
    "        \n",
    "        if curr_hops < hops:\n",
    "            # adding children\n",
    "            for item_next in curr_res.difference(visited):\n",
    "                queue.append((curr_hops + 1, item_next))\n",
    "            \n",
    "    return visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(select_adjacent_videos(video_to_change, Q(user__id=u.id) | Q(user__id=u_other.id),\n",
    "   hops=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_gin_config('../backend/backend/ml_model/config/featureless_config.gin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = DatabasePreferenceLearnerFeatureless(directory=None,\n",
    "                                     load=True, save=False,\n",
    "                                     user_queryset=UserPreferences.objects.filter(Q(id=u.id)|Q(id=u_other.id)),\n",
    "                                     users_to_ratings=users_to_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "losses = []\n",
    "times = []\n",
    "t0 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in tqdm(range(1000)):\n",
    "    info = learner.aggregator.fit_step()\n",
    "    losses.append(info['loss'])\n",
    "    times.append(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(times, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running gradient descent is too slow..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running binary search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb = learner.aggregator.sample_minibatch(sample_experts=50, sample_ratings_per_expert=10000000,\n",
    "                                   sample_objects_per_expert=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.aggregator.loss_fcn(**mb)['loss'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_id = learner.aggregator.all_ratings.experts_reverse[u.id]\n",
    "expert_id_2 = learner.aggregator.all_ratings.experts_reverse[u_other.id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_id = learner.aggregator.all_ratings.output_features.index(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_id = learner.aggregator.all_ratings.objects_reverse[rating.video_1.video_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_id_2 = learner.aggregator.all_ratings.objects_reverse[rating.video_2.video_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_id, object_id, feature_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_expert = learner.aggregator.all_ratings.aggregate_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.aggregator.all_ratings.layer.v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.aggregator.all_ratings.layer.v[expert_id, object_id, feature_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.aggregator.all_ratings.model.layers[1].v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.aggregator.loss_fcn = learner.aggregator.build_loss_fcn(**learner.aggregator.hypers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_var():\n",
    "    return learner.aggregator.all_ratings.model.layers[1].v\n",
    "def set_var(z):\n",
    "    learner.aggregator.all_ratings.model.layers[1].v = z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def get_grad_value(expert_id, object_id, feature_id, mb):\n",
    "\n",
    "#     learner.aggregator.loss_fcn = learner.aggregator.build_loss_fcn(**learner.aggregator.hypers)\n",
    "\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        losses = learner.aggregator.loss_fcn(**mb)#, model_tensor=get_var())\n",
    "\n",
    "    all_variables = learner.aggregator.all_ratings.model.variables\n",
    "    grads = tape.gradient(losses['loss'], all_variables,\n",
    "                          unconnected_gradients=tf.UnconnectedGradients.ZERO)\n",
    "    \n",
    "    grad_val = grads[0][expert_id, object_id, feature_id]#.numpy()\n",
    "    return grad_val, losses['loss']#.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def set_tensor_val(expert_id, object_id, feature_id, val, var=get_var()):\n",
    "    var.scatter_nd_update([[expert_id, object_id, feature_id]], [val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensor_val(expert_id, object_id, feature_id):\n",
    "    return get_var()[expert_id, object_id, feature_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tensor_val(expert_id, object_id, feature_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_grad_value(expert_id, object_id, feature_id, mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l, r = -10, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_tensor_val(expert_id, object_id, feature_id, l)\n",
    "get_grad_value(expert_id, object_id, feature_id, mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_tensor_val(expert_id, object_id, feature_id, r)\n",
    "get_grad_value(expert_id, object_id, feature_id, mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "grads = []\n",
    "theta_vals = []\n",
    "s_vals = []\n",
    "\n",
    "theta_vals_2 = []\n",
    "s_vals_2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_search(expert_id, object_id, feature_id, mb):\n",
    "    i = 0\n",
    "    l = -10\n",
    "    r = 10\n",
    "    while r - l >= 1e-2:\n",
    "        m = (l + r) / 2\n",
    "        set_tensor_val(expert_id, object_id, feature_id, m)\n",
    "        grad, loss = get_grad_value(expert_id, object_id, feature_id, mb)\n",
    "        if grad < 0:\n",
    "            l = m\n",
    "        else:\n",
    "            r = m\n",
    "\n",
    "        i += 1\n",
    "#         print(i, l, r, grad, loss)\n",
    "#         losses.append(loss)\n",
    "#         grads.append(grad)\n",
    "        \n",
    "#         if object_id == object_id_2:\n",
    "            \n",
    "#             theta_vals_2.append(get_tensor_val(0, object_id, feature_id))\n",
    "#         else:\n",
    "#             theta_vals.append(get_tensor_val(0, object_id, feature_id))\n",
    "\n",
    "#         if object_id == object_id_2:\n",
    "#             s_vals_2.append(get_tensor_val(1, object_id, feature_id))\n",
    "#         else:\n",
    "#             s_vals.append(get_tensor_val(1, object_id, feature_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_search_iter(mb):\n",
    "\n",
    "    binary_search(0, object_id, feature_id, mb)\n",
    "    binary_search(1, object_id, feature_id, mb)\n",
    "\n",
    "    binary_search(0, object_id_2, feature_id, mb)\n",
    "    binary_search(1, object_id_2, feature_id, mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    binary_search_iter(mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit binary_search_iter(mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(losses, label='loss')\n",
    "plt.plot(np.abs(grads), label='grad')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(theta_vals, label='theta')\n",
    "plt.plot(s_vals, label='s')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(theta_vals_2, label='theta2')\n",
    "plt.plot(s_vals_2, label='s2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r =[r for r in  learner.user_to_model[u.id].ratings if r['o1'] == rating.video_1.video_id and r['o2'] == rating.video_2.video_id][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(val):\n",
    "    r =[r for r in  learner.user_to_model[u.id].ratings if r['o1'] == rating.video_1.video_id and r['o2'] == rating.video_2.video_id][0]\n",
    "    \n",
    "    r['ratings'][feature_id] = val\n",
    "    \n",
    "    mb = learner.aggregator.sample_minibatch(sample_experts=50, sample_ratings_per_expert=1000,\n",
    "                               sample_objects_per_expert=1000)\n",
    "    \n",
    "    binary_search_iter(mb)\n",
    "\n",
    "    theta1 = get_tensor_val(0, object_id, feature_id).numpy()\n",
    "    s1 = get_tensor_val(1, object_id, feature_id).numpy()\n",
    "    \n",
    "    theta2 = get_tensor_val(0, object_id_2, feature_id).numpy()\n",
    "    s2 = get_tensor_val(1, object_id_2, feature_id).numpy()\n",
    "\n",
    "    return theta1, s1, theta2, s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(f, val=widgets.FloatSlider(min=-1, max=1, step=0.05, value=0));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fast tensor slice assign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(tf.random_normal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = tf.Variable(tf.random_uniform_initializer(minval=-1., maxval=1.)(shape=(2, 10000, 8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit variable[0, 1, 2].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def get_var_tff():\n",
    "    return variable[0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit get_var_tff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_eager():\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = tf.reduce_mean(tf.sin(tf.abs(variable)))\n",
    "    return tape.gradient(loss, variable)[0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit gradient_eager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def gradient_tff():\n",
    "    loss = tf.reduce_mean(tf.sin(tf.abs(variable)))\n",
    "    return tf.gradients(loss, [variable])[0][0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_tff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit gradient_tff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_numpy = variable.numpy()\n",
    "var_numpy_other = np.copy(var_numpy)\n",
    "var_numpy_other[0, 1, 2] = 0\n",
    "mask_one = np.zeros_like(var_numpy)\n",
    "mask_one[0,  1, 2] = 1.\n",
    "mask_one_tf = tf.constant(mask_one)\n",
    "mask_one_tf_bool = tf.constant(mask_one.astype(np.bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_changing_part = tf.Variable(variable.numpy()[0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_other_part = tf.constant(var_numpy_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit var_changing_part * mask_one_tf + var_other_part - variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def fill_rest(changing_part):\n",
    "    return tf.where(mask_one_tf_bool, var_changing_part, var_other_part)\n",
    "    #return changing_part * mask_one_tf + var_other_part\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit tf.where(mask_one_tf_bool, tf.ones_like(var_other_part) * var_changing_part, var_other_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit fill_rest(var_changing_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def gradient_slice_tff():\n",
    "    loss = tf.reduce_mean(tf.sin(tf.abs(fill_rest(var_changing_part))))\n",
    "    return tf.gradients(loss, [var_changing_part])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_slice_tff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit gradient_slice_tff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit tf.where(mask_one_tf_bool, var_changing_part, var_other_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_max(tf.abs(tf.where(mask_one_tf_bool, var_changing_part, var_other_part) - variable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_changing_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def grad_slice_and_fill_rest():\n",
    "    rec = tf.where(mask_one_tf_bool, var_changing_part, var_other_part)\n",
    "    loss = tf.reduce_mean(tf.sin(tf.abs(rec)))\n",
    "    return tf.gradients(loss, [var_changing_part])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit grad_slice_and_fill_rest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_slice_and_fill_rest():\n",
    "    with tf.GradientTape() as tape:\n",
    "        rec = tf.where(mask_one_tf_bool, var_changing_part, var_other_part)\n",
    "        loss = tf.reduce_mean(tf.sin(tf.abs(rec)))\n",
    "    return tape.gradient(loss, var_changing_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit grad_slice_and_fill_rest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable + tf.sparse.SparseTensor(indices=[[0, 1, 2]],\n",
    "                       values=[0],\n",
    "                       dense_shape=variable.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_assign_dummy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.tensor_scatter_nd_update(variable, [[0, 1, 2]], [3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable[0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit tf.compat.v1.scatter_nd_update(variable, [[0, 1, 2]], [3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def scatter_update():\n",
    "#     return tf.tensor_scatter_nd_update(variable, [[0, 1, 2]], [val])\n",
    "    tf.compat.v1.scatter_nd_update(variable, [[0, 1, 2]], [51])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit scatter_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit variable.scatter_nd_update([[0, 1, 2]], [5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def scatter_var():\n",
    "    variable.scatter_nd_update([[0, 1, 2]], [5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit scatter_var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = np.prod(variable.shape)\n",
    "lst = np.random.randn(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHY IS NUMPY SO MUCH FASTER?????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit lst[np.random.choice(N)] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit lst[np.random.choice(N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit tf.gather_nd(variable, [[0, 1, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying tf1.0... -- even worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = tf.Variable(tf.random_uniform_initializer(minval=-1., maxval=1.)(shape=(2, 10000, 8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.compat.v1.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.run(tf.compat.v1.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit session.run(variable[0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying torch..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = torch.autograd.Variable(torch.randn(2, 10000, 8), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit var[0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit var[0, 1, 2] = np.random.randn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad():\n",
    "    var.grad = None\n",
    "    loss = torch.mean(torch.sin(torch.abs(var)))\n",
    "    loss.backward()\n",
    "    return var.grad.data[0, 1, 2].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit compute_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.zeros_like(var, dtype=torch.bool)\n",
    "mask[0, 1, 2]=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var[0, 1, 2] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_single = torch.autograd.Variable(torch.from_numpy(np.array(var[0, 1,2].item()).astype(np.float32)), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_rest = torch.from_numpy(var.detach().numpy().copy()).to(torch.float)\n",
    "const_rest[0, 1, 2] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var[0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_rest + var_single - var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit torch.where(mask, var_single, const_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad():\n",
    "    var_single.grad = None\n",
    "    var1 = torch.where(mask, var_single, const_rest)\n",
    "    loss = torch.mean(torch.sin(torch.abs(var1)))\n",
    "    loss.backward()\n",
    "    return var_single.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit compute_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign is slower on tf but manageable??\n",
    "# ~800mus anyway spent on gradient computation and loss computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide into 3 parts, no grad\n",
    "def compute_loss():\n",
    "    loss = torch.mean(torch.sin(torch.abs(var)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit compute_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# much faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_index_layer_call_torch(model, idxes1, idxes2):\n",
    "    return model[idxes1, idxes2]\n",
    "\n",
    "def loss_fcn_torch(\n",
    "        experts_rating=None,\n",
    "        objects_rating_v1=None,\n",
    "        objects_rating_v2=None,\n",
    "        cmp=None,\n",
    "        weights=None,\n",
    "        experts_all=None,\n",
    "        objects_all=None,\n",
    "        num_ratings_all=None,\n",
    "        objects_common_to_1=None,\n",
    "        model_tensor=None,\n",
    "        aggregate_index=None,\n",
    "        lambda_=None,\n",
    "        mu=None,\n",
    "        C=None,\n",
    "        default_score_value=None,\n",
    "        **kwargs):\n",
    "    \"\"\"\n",
    "    Compute the loss function. All IDs are internal (int64).\n",
    "\n",
    "    See https://www.overleaf.com/project/5f44dd8e84c8540001bf1552\n",
    "    Equations 1-2-3\n",
    "\n",
    "    Args:\n",
    "        experts_rating: 1D tensor with expert IDs\n",
    "        objects_rating_v1: 1D tensor with LEFT objects, same length as experts_rating\n",
    "        objects_rating_v2: 1D tensor with RIGHT objects, same length as experts_rating\n",
    "        cmp: 2D tensor comparison_id, feature_id, same length as experts_rating\n",
    "        weights: 2D tensor comparison_id, feature_weight, same length as experts_rating\n",
    "        experts_all: 1D tensor with expert IDs for the regularization loss\n",
    "        objects_all: 1D tensor with objects (for common loss), same length as experts_all\n",
    "        num_ratings_all: 1D tensor with number of ratings for expert/object in experts_all\n",
    "            and objects_all, same length as experts_all\n",
    "        objects_common_to_1: 1D tensor with object IDs for the common-to-1 loss.\n",
    "\n",
    "    Returns dict of Tensorflow tensors with the total loss and components\n",
    "    \"\"\"\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    # internal indices for experts and objects (ratings)\n",
    "#     idx_v1 = torch.stack((experts_rating, objects_rating_v1), dim=1)\n",
    "#     idx_v2 = torch.stack((experts_rating, objects_rating_v2), dim=1)\n",
    "\n",
    "#     print(idx_v1)\n",
    "    \n",
    "    # 2D array (comparison_id, feature) -> float\n",
    "    theta_eqv = variable_index_layer_call_torch(model_tensor, experts_rating, objects_rating_v1)\n",
    "    theta_eqw = variable_index_layer_call_torch(model_tensor, experts_rating, objects_rating_v2)\n",
    "\n",
    "    # FIT LOSS SUM\n",
    "    theta_vw = theta_eqv - theta_eqw\n",
    "    # print(theta_vw.shape, cmp.shape)\n",
    "    theta_vw_y = torch.mul(theta_vw, cmp)\n",
    "    sp = torch.nn.Softplus()(theta_vw_y)\n",
    "    sp_weighted = torch.mul(sp, weights)\n",
    "\n",
    "    sp_weighted_flat = sp_weighted.view((-1,))\n",
    "    sp_weighted_no_nan = sp_weighted_flat[~torch.isnan(sp_weighted_flat)]\n",
    "    # tf.print(\"original tensor\")\n",
    "    # tf.print(sp_weighted_flat)\n",
    "\n",
    "    # tf.print(\"nonan tensor\")\n",
    "    # tf.print(sp_weighted_no_nan)\n",
    "\n",
    "    result['loss_fit'] = torch.sum(sp_weighted_no_nan)\n",
    "\n",
    "    # LOSS MODEL TO COMMON\n",
    "    # common expert\n",
    "    experts_common = torch.full(experts_all.shape, aggregate_index, dtype=torch.int64)\n",
    "\n",
    "    # indices for experts for regularization\n",
    "#     idx_all = torch.stack((experts_all, objects_all), dim=1)\n",
    "#     idx_common = torch.stack((experts_common, objects_all), dim=1)\n",
    "\n",
    "    # 2D array (regul_id, feature) -> float\n",
    "    theta_eqv_common = variable_index_layer_call_torch(model_tensor, experts_all, objects_all)\n",
    "    s_qv = variable_index_layer_call_torch(model_tensor, experts_common, objects_all)\n",
    "\n",
    "    # print(\"IDX\", idx_common.shape, s_qv.shape)\n",
    "\n",
    "    # coefficient, shape: regul_id\n",
    "    num_float = num_ratings_all.to(torch.float32)\n",
    "    coef_yev = torch.div(num_float, C + num_float)\n",
    "    # print(theta_eqv_common.shape, s_qv.shape)\n",
    "    # tf.print('thetacomm', theta_eqv_common)\n",
    "    # tf.print(\"sqv\", s_qv)\n",
    "    theta_s = torch.abs(theta_eqv_common - s_qv)\n",
    "    coef_yev_repeated = torch.unsqueeze(\n",
    "            coef_yev,\n",
    "            dim=1).repeat(\n",
    "        1,\n",
    "        theta_s.shape[1])\n",
    "    # print(\"THETAS\", theta_s.shape, \"COEF\", coef_yev.shape, \\\n",
    "    # \"COEFR\", coef_yev_repeated.shape)\n",
    "    # tf.print(\"thetas\", theta_s)\n",
    "    # tf.print(\"coefyev\", coef_yev_repeated)\n",
    "    theta_s_withcoeff = torch.mul(theta_s, coef_yev_repeated)\n",
    "    result['loss_m_to_common'] = torch.sum(\n",
    "        theta_s_withcoeff) * lambda_\n",
    "\n",
    "    \n",
    "    \n",
    "    # LOSS COMMON TO 0\n",
    "    experts_common_to_1 = torch.full(objects_common_to_1.shape,\n",
    "                                     aggregate_index, dtype=torch.int64)\n",
    "#     idx_common_to_1 = torch.stack(\n",
    "#         (experts_common_to_1, objects_common_to_1), dim=1)\n",
    "    s_qv_common_to_1 = variable_index_layer_call_torch(model_tensor, experts_common_to_1, objects_common_to_1)\n",
    "\n",
    "    sm1 = torch.pow(s_qv_common_to_1 - default_score_value, 2.0)\n",
    "\n",
    "    # print(idx_common_to_1, s_qv_common_to_1, sm1)\n",
    "\n",
    "    result['loss_common_to_1'] = torch.sum(sm1) * mu\n",
    "\n",
    "    # TOTAL LOSS\n",
    "    result['loss'] = result['loss_fit'] + result['loss_m_to_common'] + result[\n",
    "        'loss_common_to_1']\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_np = {x: y.numpy() for x, y in mb.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_model = var.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fcn = loss_fcn_np(**mb_np, **learner.aggregator.hypers, model_tensor=np_model)['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit loss_fcn_np(**mb_np, **learner.aggregator.hypers, model_tensor=np_model)['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -++ # delete rightmost\n",
    "# --+ # delete leftmost\n",
    "# +-- # delete leftmost\n",
    "# ++- # delete rightmost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trisection(fcn, l, r, get_var, set_var, eps=1e-3):\n",
    "    def get_m1_m2(l, r):\n",
    "        delta = (r - l) / 3.\n",
    "        m1 = l + delta\n",
    "        m2 = l + 2 * delta\n",
    "        return m1, m2\n",
    "    while r - l > eps:\n",
    "        m1, m2 = get_m1_m2(l, r):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import golden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?golden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    np_model[1, 5, 6] = x\n",
    "    return loss_fcn_np(**mb_np, **learner.aggregator.hypers, model_tensor=np_model)['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit golden(func, full_output=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?golden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden(func, full_output=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.ml_model.preference_aggregation_featureless_online import FeaturelessOnlineUpdater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%timeit learner.aggregator.loss_fcn(**mb)['loss'].numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_torch = {x: torch.from_numpy(y.numpy()) for x, y in mb.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.aggregator.hypers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = torch.autograd.Variable(torch.from_numpy(learner.aggregator.all_ratings.layer.v.numpy()),\n",
    "                                      requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fcn_torch(**mb_torch, **learner.aggregator.hypers, model_tensor=torch_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.aggregator.loss_fcn(**mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit learner.aggregator.loss_fcn(**mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit loss_fcn_torch(**mb_torch, **learner.aggregator.hypers, model_tensor=torch_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = learner.aggregator.all_ratings.model.variables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = learner.aggregator.build_loss_fcn(**learner.aggregator.hypers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def grad_compute_tf():\n",
    "#     del mb['']\n",
    "    loss = loss_fn(**mb)['loss']\n",
    "    grad = tf.gradients(loss, [var])[0]\n",
    "    return grad[1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit grad_compute_tf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_compute_tf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_compute_torch():\n",
    "    torch_model.grad = None\n",
    "    loss = loss_fcn_torch(**mb_torch, **learner.aggregator.hypers, model_tensor=torch_model)['loss']\n",
    "    loss.backward()\n",
    "    return torch_model.grad.data[1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit grad_compute_torch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = learner.aggregator.all_ratings.layer.v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var[0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit var.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_var = var.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_var[0, 1, 2] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var[0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_const = tf.constant(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nparr = np.asarray(memoryview(var_const))\n",
    "nparr.setflags(write=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_const[0, 1, 2] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nparr[0, 1, 2] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var[0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nparr[0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.prod(np_model.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1000*5000*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2*1000*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.aggregator.hypers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gin.query_parameter('FeaturelessMedianPreferenceAverageRegularizationAggregator.hypers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Updater class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from django_react.settings import load_gin_config\n",
    "load_gin_config('../backend/backend/ml_model/config/featureless_config.gin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "from backend.ml_model.preference_aggregation_featureless_online import FeaturelessOnlineUpdater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FILL DATA FROM LEARNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online = FeaturelessOnlineUpdater(golden_params={'maxiter': 20, 'tol': 1e-8, 'smartbracket': (-1, 1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online = FeaturelessOnlineUpdater(golden_params={'maxiter': 20, 'tol': 1e-8, 'smartbracket': (-1, 1)})\n",
    "online.hypers['aggregate_index'] = common_expert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('sdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online.set_minibatch({x: y.numpy() for x, y in mb.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online.set_model_tensor(learner.aggregator.all_ratings.model.variables[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online.set_subtract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_lst = [\n",
    "    (expert_id,     object_id,   feature_id),\n",
    "    (expert_id,     object_id_2, feature_id),\n",
    "    \n",
    "    (expert_id_2,   object_id,   feature_id),\n",
    "    (expert_id_2,   object_id_2, feature_id),\n",
    "    \n",
    "    (common_expert, object_id,   feature_id),\n",
    "    (common_expert, object_id_2, feature_id),\n",
    "] * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_value = {ind: online.get_closure_loss(ind)(online.get_value(ind)) for ind in set(indices_lst)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online.get_value(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = online.best_value_many_indices(indices_lst, assign_at_end=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit result = online.best_value_many_indices(indices_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usernames = [my_username, other_username]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usernames to update\n",
    "ups = get_from_list(UserPreferences.objects.all(), 'user__username', usernames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtaining video to rate\n",
    "video1 = VideoRating.objects.filter(user__user__username=my_username).order_by('?')[0].video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video2 = ExpertRating.objects.filter(user__user__username=my_username, video_1=video1).order_by('?')[0].video_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FILL DATA FROM DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online = FeaturelessOnlineUpdater()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_set_value = 'test_other_username'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_to_change = ExpertRating.objects.filter(user__user__username=user_set_value).order_by('?')[0]\n",
    "video1 = rating_to_change.video_1\n",
    "video2 = rating_to_change.video_2\n",
    "field_to_change = VIDEO_FIELDS[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_context = OnlineRatingUpdateContext(rating_to_change, field_to_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_np_orig = deepcopy(mb_np)\n",
    "model_tensor_orig = deepcopy(model_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_context.model_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gin.bind_parameter('FeaturelessOnlineUpdater.golden_params',\n",
    "                  {'maxiter': 20, 'tol': 1e-8, 'smartbracket': (-1, 1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "from functools import partial\n",
    "import time\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### Online updates demo\")\n",
    "\n",
    "print(\"Changing\", field_to_change, \"on\", video1, '/', video2)\n",
    "print(\"Rating by\", user_set_value, \"showing rating by\", usernames[user_get_value])\n",
    "print(\"Other users\", len(usernames) - 1)\n",
    "print(\"Ratings in the loss\", len(ratings_selected))\n",
    "print(\"Videos in the loss\", len(videos_selected))\n",
    "\n",
    "\n",
    "pbar = IntProgress(min=0, max=100)\n",
    "display(pbar) # display the bar\n",
    "\n",
    "fcn = partial(compute_online_update, pbar=pbar)\n",
    "fcn.__name__ = \"Online updates\"\n",
    "interact(fcn, rating_value=widgets.FloatSlider(min=-1, max=1, step=0.05, value=0),\n",
    "         pbar=fixed(pbar), mb_np_orig=fixed(mb_np_orig), model_tensor_orig=fixed(model_tensor_orig),\n",
    "         idx_set=fixed(idx_set), user_get_value=fixed(-1),#,fixed(user_get_value),\n",
    "         maxiter=20, n_repeats=1, tol=fixed(1e-8),\n",
    "         hotfix_update_hypers=fixed({'mu': 1, 'lambda_': 0.1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
