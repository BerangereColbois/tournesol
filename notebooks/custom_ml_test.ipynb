{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for adding the videos to DB\n",
    "# don't use at the same time with the server running\n",
    "# https://stackoverflow.com/questions/59119396/how-to-use-django-3-0-orm-in-a-jupyter-notebook-without-triggering-the-async-con\n",
    "import os\n",
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\"\n",
    "#from backend.ml_models import DatabasePreferenceLearner\n",
    "import numpy as np\n",
    "\n",
    "from backend.models import Video, UserPreferences, DjangoUser, VideoRating\n",
    "from backend.rating_fields import VIDEO_FIELDS\n",
    "\n",
    "#%pylab\n",
    "%matplotlib inline\n",
    "#%matplotlib widget\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import mplcursors\n",
    "\n",
    "import sys\n",
    "import requests\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import datetime\n",
    "from uuid import uuid1\n",
    "import shortuuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "## Useful functions to generate fictitious data\n",
    "\n",
    "# Takes any real z and outputs a number between 0 and 1\n",
    "def logisticFunction(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "# This assumes that if video1 > video2, then the rating is closer to rating_min\n",
    "def fullScore(user, video1, video2, scores, rating_min, rating_max):\n",
    "    score_difference = scores[user][video1] - scores[user][video2]\n",
    "    if (np.random.uniform() < logisticFunction(score_difference)):\n",
    "        return rating_min\n",
    "    return rating_max\n",
    "\n",
    "# This assumes that if video1 > video2, then the rating is closer to rating_min\n",
    "def intermediateScore(user, video1, video2, scores, rating_min, rating_max, intermediate_score_noise):\n",
    "    score_difference = scores[user][video1] - scores[user][video2]\n",
    "    rating_between_zero_one = 1-logisticFunction(score_difference)\n",
    "    rating = (rating_max - rating_min) * rating_between_zero_one + rating_min\n",
    "    noisy_rating = rating + intermediate_score_noise * (rating_max - rating_min) * np.random.normal()\n",
    "    return min(rating_max, max(noisy_rating,rating_min))\n",
    "\n",
    "def video_pair_recursive(video_pair, nb_remaining):\n",
    "    if video_pair < nb_remaining - 1:\n",
    "        return (video_pair, nb_remaining-1)\n",
    "    (video1_temp, video2_temp) = video_pair_recursive(1+video_pair - nb_remaining, nb_remaining-1)\n",
    "    return (video1_temp, video2_temp)\n",
    "\n",
    "def video_pair_to_video1_video2(video_pair, nb_videos):\n",
    "    if video_pair >= nb_videos * (nb_videos-1) /2:\n",
    "        return (-1,-1) # Error\n",
    "    return video_pair_recursive(video_pair, nb_videos)\n",
    "\n",
    "def random_video1_video2(nb_videos):\n",
    "    return video_pair_to_video1_video2(np.random.randint(nb_videos * (nb_videos-1)/2), nb_videos)\n",
    "\n",
    "def random_user(nb_users, user_skewness):\n",
    "    return int(nb_users * (np.random.uniform() ** (user_skewness)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming true_scores and learned_scores have as many users\n",
    "def individual_scores_divergence(true_scores, learned_scores):\n",
    "    sum_of_squares = 0\n",
    "    nb_comparisons = 0\n",
    "    for user in len(true_scores):\n",
    "        for video in len(true_scores[0]):\n",
    "            if learned_scores[user].has_key(video):\n",
    "                sum_of_squares = sum_of_squares + (learned_scores[user][video] - true_scores[user][video])**2\n",
    "                nb_comparisons = nb_comparisons + 1\n",
    "    return np.sqrt(sum_of_squares / nb_comparisons)\n",
    "    \n",
    "def median_score(true_scores, video):\n",
    "    list_scores = []\n",
    "    for user in range(len(true_scores)):\n",
    "        list_scores.append(true_scores[user][video])\n",
    "    return np.median(list_scores)\n",
    "    \n",
    "# Assuming true_scores and learned_scores have as many users\n",
    "def aggregate_scores_divergence(true_scores, tournesol_scores):\n",
    "    sum_of_squares = 0\n",
    "    for video in len(tournesol_scores):\n",
    "        sum_of_squares = sum_of_squares + (median_score(true_scores,video) - tournesol_scores[video])**2\n",
    "    return np.sqrt(sum_of_squares / len(tournesol_scores))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters of the test dataset\n",
    "nb_users = 10\n",
    "nb_videos = 100\n",
    "\n",
    "# Larger skewness means some users rated a lot more videos than others\n",
    "# Skewness = 1 means that all users contributed equally\n",
    "user_skewness = 2                  \n",
    "\n",
    "# Sparsity of user inputs: smaller sparsity makes learning trickier\n",
    "sparsity = .5\n",
    "\n",
    "# Variance between the true scores of different users for the same video\n",
    "score_variance_per_user = 2\n",
    "\n",
    "# This assumes that the ratings range between 0 and rating_max\n",
    "rating_min, rating_max = 0, 100    \n",
    "\n",
    "# These are the minimum and maximum scores   \n",
    "score_min, score_max = 1, 5        \n",
    "\n",
    "# Users may provide min/max ratings, called full scores, or intermediate ratings\n",
    "prob_fullScore = .5\n",
    "\n",
    "# Larger intermediate score noises makes data noisier, and learning harder\n",
    "intermediate_score_noise = .05\n",
    "\n",
    "# This generates intrinsic scores for the videos\n",
    "video_scores = np.random.uniform(score_min, score_max, nb_videos)\n",
    "\n",
    "    \n",
    "nb_datapoints_max = nb_users * nb_videos * (nb_videos-1) / 2\n",
    "nb_datapoints = int(sparsity * nb_datapoints_max)\n",
    "\n",
    "def generate_dataset():\n",
    "    dataset = {}\n",
    "\n",
    "    # This computes the users' scores for the videos that the ML model must partially recover\n",
    "    scores = []\n",
    "    for user in range(nb_users):\n",
    "        scores.append(video_scores + score_variance_per_user * np.random.normal())\n",
    "\n",
    "\n",
    "    for i in range(nb_datapoints):\n",
    "        user = random_user(nb_users, user_skewness)\n",
    "        video1, video2 = random_video1_video2(nb_videos)\n",
    "        score_diff = scores[user][video1] - scores[user][video2]\n",
    "        if (np.random.uniform() < prob_fullScore):\n",
    "            dataset[user,video1,video2] = fullScore(user, video1, video2, scores, rating_min, rating_max)\n",
    "        else: \n",
    "            dataset[user,video1,video2] = intermediateScore(user, video1, video2, scores, rating_min, rating_max, intermediate_score_noise)\n",
    "\n",
    "    return {'dataset': dataset, 'scores': scores}\n",
    "\n",
    "gen = generate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gen['dataset']), len(gen['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.array(list(gen['dataset'].values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear everything\n",
    "UserPreferences.objects.all().delete()\n",
    "Video.objects.all().delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = shortuuid.ShortUUID().random(length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = VIDEO_FIELDS[0]\n",
    "f_default = {k: 50 for k in VIDEO_FIELDS if k != f}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = []\n",
    "for v in range(nb_videos):\n",
    "    v = Video.objects.create(video_id=prefix + str(v), name=\"test video\",\n",
    "                         views=10, duration=datetime.timedelta(seconds=5), publication_date=datetime.datetime.now())\n",
    "    videos.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = []\n",
    "for u in range(nb_users):\n",
    "    u = DjangoUser.objects.create(username=f\"{prefix}_test_user__{u}\", email=f\"{u}@tournesol.com\")\n",
    "    users.append(u)\n",
    "users = [UserPreferences.objects.create(user=u) for u in users]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = []\n",
    "for (u, v1, v2), s in gen['dataset'].items():\n",
    "    r = ExpertRating(user=users[u], video_1=videos[v1], video_2=videos[v2],\n",
    "                    **f_default, **{f: s})\n",
    "    ratings.append(r)\n",
    "ratings = ExpertRating.objects.bulk_create(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from django_react.settings import load_gin_config\n",
    "from backend.management.commands.ml_train import learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_gin_config('backend/ml_model/config/featureless_config.gin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_obj = learner()()\n",
    "learner_obj.fit()\n",
    "learner_obj.update_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_obj.aggregator.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen['scores'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_out = [[getattr(VideoRating.objects.get(user=u, video=v), f) for v in videos] for u in users]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_out_aggr = [getattr(Video.objects.get(id=v.id), f) for v in videos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "plt.title('Dataset and predicted scores')\n",
    "for u in range(len(users)):\n",
    "    plt.scatter(gen['scores'][u], scores_out[u], label=f'User {u}')\n",
    "plt.xlabel('Dataset scores')\n",
    "plt.ylabel('Predicted scores')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tournesol scores learned by the model are in average less than .5 from the true scores of the videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_loss(a, b):\n",
    "    \"\"\" For given a, b compute the average number of misordered pairs, O(n^2) \"\"\"\n",
    "    \n",
    "    # flattening data\n",
    "    a, b = np.array(a).flatten(), np.array(b).flatten()\n",
    "    \n",
    "    # checking shape\n",
    "    assert len(a) == len(b), \"Lengths must agree\"\n",
    "    \n",
    "    # sorting b in order of a\n",
    "    b = np.array(b)[np.argsort(a)]\n",
    "    \n",
    "    # number of bad pairs\n",
    "    res = sum([sum([1 if i < j and x >= y else 0 for j, y in enumerate(b)]) for i, x in enumerate(b)])\n",
    "    \n",
    "    # total number of pairs\n",
    "    NN = len(a) * (len(a) - 1) / 2\n",
    "    \n",
    "    # return the ratio\n",
    "    return 1. * res / NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in range(len(users)):\n",
    "    rl = 100 * rank_loss(gen['scores'][u], scores_out[u])\n",
    "    print(f\"User {u} rank loss {round(rl, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "plt.title('Dataset and predicted scores')\n",
    "plt.scatter(video_scores, scores_out_aggr, label=f'Aggregated')\n",
    "plt.xlabel('Dataset scores')\n",
    "plt.ylabel('Predicted scores')\n",
    "plt.legend()\n",
    "\n",
    "rl = 100 * rank_loss(video_scores, scores_out_aggr)\n",
    "print(f\"User {u} rank loss {round(rl, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
